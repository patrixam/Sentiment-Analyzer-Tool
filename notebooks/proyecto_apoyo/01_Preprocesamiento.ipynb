{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOiSewVi+nDM2lVtqr2R1sS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/patrixam/Sentiment-Analyzer-Tool/blob/master/notebooks/proyecto_apoyo/01_Preprocesamiento.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Limpieza Texto"
      ],
      "metadata": {
        "id": "9BNDjJYrhiiX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQwTFYE3hVj3",
        "outputId": "604bdbbb-ee70-4337-c4b6-1f6ab2a34b7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hola visita  \n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def limpiar_texto(texto):\n",
        "    texto = re.sub(r\"http\\S+|www\\S+\", \"\", texto)  # Eliminar URLs\n",
        "    texto = re.sub(r\"[^a-zA-Z\\s]\", \"\", texto)  # Eliminar caracteres especiales\n",
        "    return texto.lower()  # Convertir a minúsculas\n",
        "\n",
        "print(limpiar_texto(\"¡Hola! Visita https://example.com 🎉\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tokenizacion"
      ],
      "metadata": {
        "id": "AqiHbC1YvZL1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##spacy"
      ],
      "metadata": {
        "id": "Mpau8fbrtr9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy"
      ],
      "metadata": {
        "id": "0ntWOCcN5ltM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#descargar es_core_news_sm\n",
        "!spacy download es_core_news_sm"
      ],
      "metadata": {
        "id": "AJhFjiwB5XU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"es_core_news_sm\")\n",
        "texto = \"Hola, ¿cómo estás?\"\n",
        "doc = nlp(texto)\n",
        "\n",
        "tokens = [token.text for token in doc]\n",
        "print(tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhBuAoCnh9HH",
        "outputId": "d292240b-ef9c-47a9-eb22-74ad89645566"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hola', ',', '¿', 'cómo', 'estás', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##nltk"
      ],
      "metadata": {
        "id": "pBwdIbdTtumw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "id": "ASpDfsaR6gzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt_tab')  # Descargar datos necesarios para tokenización\n",
        "\n",
        "texto = \"Hola, ¿cómo estás?\"\n",
        "tokens = word_tokenize(texto)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LU2Rxrtt5FN3",
        "outputId": "ea467433-e4ca-4841-a1b8-dac8b582569b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hola', ',', '¿cómo', 'estás', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Stopwords"
      ],
      "metadata": {
        "id": "poIYwp3Qt3yn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##spacy"
      ],
      "metadata": {
        "id": "q8qrAVrb7bxj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Elimina stopwords\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"es_core_news_sm\")\n",
        "texto = \"¡Hola a todos! ¿Cuál es la dirección del sitio web www.example.com? Gracias.\"\n",
        "doc = nlp(texto)\n",
        "\n",
        "tokens = [token.text for token in doc if not token.is_stop]\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qAnVDW-4v2J",
        "outputId": "c4912a0f-c3f2-4180-c7db-3d2605782269"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['¡', 'Hola', '!', '¿', 'dirección', 'sitio', 'web', 'www.example.com', '?', 'Gracias', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##nltk"
      ],
      "metadata": {
        "id": "OTNZdwV_7Zni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Elimina stopswords\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "# Descargar datos necesarios para tokenización\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "texto = \"¡Hola a todos! ¿Cuál es la dirección del sitio web www.example.com? Gracias.\"\n",
        "tokens = word_tokenize(texto)\n",
        "stop_words = set(stopwords.words(\"spanish\"))\n",
        "tokens_filtrados = [palabra for palabra in tokens if palabra not in stop_words]\n",
        "print(tokens_filtrados)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iX8ImfOh69Q4",
        "outputId": "50448fac-5a76-48dd-fd91-4f232301847c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['¡Hola', '!', '¿Cuál', 'dirección', 'sitio', 'web', 'www.example.com', '?', 'Gracias', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Normalización"
      ],
      "metadata": {
        "id": "geefRIxLt6UO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Stemming"
      ],
      "metadata": {
        "id": "yNL_Lbbdvk36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')  # Descargar datos necesarios para tokenización\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Inicializar el stemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Texto de ejemplo\n",
        "texto = \"Corriendo, corrí rápidamente y los corredores corrían.\"\n",
        "\n",
        "# Tokenizar el texto\n",
        "tokens = word_tokenize(texto)\n",
        "\n",
        "# Aplicar stemming\n",
        "stems = [stemmer.stem(token) for token in tokens]\n",
        "print(\"Palabras originales:\", tokens)\n",
        "print(\"Raíces (stems):\", stems)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6MN5X9giHRx",
        "outputId": "bb72cd90-b9c6-4940-a460-22b228fde613"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Palabras originales: ['Corriendo', ',', 'corrí', 'rápidamente', 'y', 'los', 'corredores', 'corrían', '.']\n",
            "Raíces (stems): ['corriendo', ',', 'corrí', 'rápidament', 'y', 'lo', 'corredor', 'corrían', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Lematizacion"
      ],
      "metadata": {
        "id": "4hJoq2liwfAC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"es_core_news_sm\")\n",
        "\n",
        "# Texto de ejemplo\n",
        "texto = \"Corriendo, corrí rápidamente y los corredores corrían.\"\n",
        "doc = nlp(texto)\n",
        "\n",
        "# Aplicar lematizacion\n",
        "lemas = [token.lemma_ for token in doc]\n",
        "\n",
        "print(\"Palabras originales:\", tokens)\n",
        "print(\"Lemas:\", lemas)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vV04I4Ywgc6",
        "outputId": "94eb3d06-3b93-48ef-b6ef-4cc5f58f0685"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Palabras originales: ['Corriendo', ',', 'corrí', 'rápidamente', 'y', 'los', 'corredores', 'corrían', '.']\n",
            "Lemas: ['Corriendo', ',', 'corrí', 'rápidamente', 'y', 'el', 'corredor', 'corer', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ejemplo Completo"
      ],
      "metadata": {
        "id": "ltAhIwCRwRA3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()\n",
        "nlp = spacy.load(\"es_core_news_sm\")\n",
        "\n",
        "# Corpus de ejemplo\n",
        "corpus = [\n",
        "    \"¡Hola! ¿Cómo estás? Visita https://example.com para más información.\",\n",
        "    \"Me encanta este curso de PLN.\",\n",
        "    \"¿Dónde están las oficinas?\",\n",
        "    \"En el 2024, implementaremos más IA.\"\n",
        "]\n",
        "\n",
        "# Pipeline completo\n",
        "def preprocesar(texto):\n",
        "    # Limpieza\n",
        "    texto = re.sub(r\"http\\S+|www\\S+\", \"\", texto)  # Eliminar URLs\n",
        "    texto = re.sub(r\"\\d+\", \"\", texto)  # Eliminar números\n",
        "    texto = re.sub(r\"[^a-zA-ZáéíóúñÑ\\s]\", \"\", texto)  # Eliminar caracteres especiales\n",
        "    texto = texto.lower()  # Minúsculas\n",
        "\n",
        "    # Tokenización\n",
        "    tokens = word_tokenize(texto)\n",
        "\n",
        "    # Stemming\n",
        "    stop_words = set(stopwords.words(\"spanish\"))\n",
        "    stems = [stemmer.stem(token) for token in tokens if token not in stop_words]\n",
        "\n",
        "    # Lematización\n",
        "    doc = nlp(texto)\n",
        "    lemas = [token.lemma_ for token in doc if not token.is_stop]\n",
        "\n",
        "    return {\"original\": texto, \"tokens\": tokens, \"stems\": stems, \"lemas\": lemas}\n",
        "\n",
        "# Aplicar pipeline al corpus\n",
        "resultados = [preprocesar(texto) for texto in corpus]\n",
        "for i, res in enumerate(resultados):\n",
        "    print(f\"Texto {i+1}:\")\n",
        "    print(\"Original:\", res[\"original\"])\n",
        "    print(\"Tokens:\", res[\"tokens\"])\n",
        "    print(\"Stems:\", res[\"stems\"])\n",
        "    print(\"Lemas:\", res[\"lemas\"])\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RbNCAaliLLS",
        "outputId": "638069a0-28cf-4f25-a37c-77de85f5bbb3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto 1:\n",
            "Original: hola cómo estás visita  para más información\n",
            "Tokens: ['hola', 'cómo', 'estás', 'visita', 'para', 'más', 'información']\n",
            "Stems: ['hola', 'cómo', 'visita', 'información']\n",
            "Lemas: ['hola', 'estar', 'visita', ' ', 'información']\n",
            "--------------------------------------------------\n",
            "Texto 2:\n",
            "Original: me encanta este curso de pln\n",
            "Tokens: ['me', 'encanta', 'este', 'curso', 'de', 'pln']\n",
            "Stems: ['encanta', 'curso', 'pln']\n",
            "Lemas: ['encantar', 'curso', 'pln']\n",
            "--------------------------------------------------\n",
            "Texto 3:\n",
            "Original: dónde están las oficinas\n",
            "Tokens: ['dónde', 'están', 'las', 'oficinas']\n",
            "Stems: ['dónde', 'oficina']\n",
            "Lemas: ['oficina']\n",
            "--------------------------------------------------\n",
            "Texto 4:\n",
            "Original: en el  implementaremos más ia\n",
            "Tokens: ['en', 'el', 'implementaremos', 'más', 'ia']\n",
            "Stems: ['implementaremo', 'ia']\n",
            "Lemas: [' ', 'implementaremo', 'ia']\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Corregir Ortografía"
      ],
      "metadata": {
        "id": "iXmduvj_nMpL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Hunspell"
      ],
      "metadata": {
        "id": "WzFHyNl_oJKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#instalar hunspell\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install libhunspell-dev hunspell\n",
        "!pip install hunspell\n"
      ],
      "metadata": {
        "id": "LxTbS-5EoMSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#instalar diccionario\n",
        "!sudo apt-get install hunspell-es\n"
      ],
      "metadata": {
        "id": "JLv-4nwyr91Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import hunspell\n",
        "\n",
        "# Inicializa Hunspell con el diccionario español\n",
        "hspell = hunspell.HunSpell('/usr/share/hunspell/es_ES.dic', '/usr/share/hunspell/es_ES.aff')\n",
        "\n",
        "# Texto original\n",
        "texto = \"corrienddo rapidamente los corredoress corian\"\n",
        "\n",
        "# Dividir el texto en palabras\n",
        "palabras = texto.split()\n",
        "\n",
        "# Corregir cada palabra\n",
        "correcciones = [\n",
        "    hspell.suggest(palabra)[0] if not hspell.spell(palabra) and hspell.suggest(palabra) else palabra\n",
        "    for palabra in palabras\n",
        "]\n",
        "\n",
        "# Texto corregido\n",
        "texto_corregido = \" \".join(correcciones)\n",
        "print(\"Texto corregido:\", texto_corregido)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBoP-TWkskrJ",
        "outputId": "4844fc21-6cef-448b-d835-2a7b26dbb302"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto corregido: corriendo rápidamente los corredores coran\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##SymSpell"
      ],
      "metadata": {
        "id": "0UJFXlXynueT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install symspellpy"
      ],
      "metadata": {
        "id": "ZRUZp5Stn0X-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from symspellpy import SymSpell, Verbosity\n",
        "import requests as request\n",
        "\n",
        "#descargo un archivo de frecuencias de Wikipedia en español\n",
        "response = request.get(\"https://raw.githubusercontent.com/hermitdave/FrequencyWords/master/content/2018/es/es_50k.txt\")\n",
        "with open(\"frequency_dictionary_es.txt\", \"w\") as f:\n",
        "    f.write(response.text)\n",
        "    f.close()\n",
        "# Inicializar SymSpell\n",
        "sym_spell = SymSpell(max_dictionary_edit_distance=2)\n",
        "sym_spell.load_dictionary(\"frequency_dictionary_es.txt\", term_index=0, count_index=1)\n",
        "\n",
        "# Corregir texto\n",
        "texto = \"corrienddo rapdamente los corredoress corian\"\n",
        "correcciones = [sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=2)[0].term\n",
        "                if sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=2) else word\n",
        "                for word in texto.split()]\n",
        "print(\"Texto corregido:\", \" \".join(correcciones))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxUvMQ8xnxpF",
        "outputId": "6ba4401e-7543-41e6-aefd-42532882db1f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto corregido: corriendo rapidamente los corredores corran\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Pyspellchecker"
      ],
      "metadata": {
        "id": "TomHxvRCoa2X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyspellchecker\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCGUdIuBoiB2",
        "outputId": "5fb78686-e15a-4289-c77f-1ada53673e75"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspellchecker\n",
            "  Downloading pyspellchecker-0.8.3-py3-none-any.whl.metadata (9.5 kB)\n",
            "Downloading pyspellchecker-0.8.3-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyspellchecker\n",
            "Successfully installed pyspellchecker-0.8.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spellchecker import SpellChecker\n",
        "\n",
        "# Inicializar para español\n",
        "spell = SpellChecker(language='es')\n",
        "\n",
        "# Palabras a corregir\n",
        "texto = \"corrienddo rapidamente los corredoress corian\"\n",
        "tokens = texto.split()\n",
        "\n",
        "# Corrección ortográfica\n",
        "correcciones = [spell.correction(token) for token in tokens]\n",
        "print(\"Texto corregido:\", correcciones)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkbf31Wpod73",
        "outputId": "b2e27bcd-d2d7-422b-8924-5a2dee623f26"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto corregido: ['correinado', 'rápidamente', 'los', None, 'coria']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtener sugerencias para una palabra incorrecta\n",
        "palabra = \"corrienndo\"\n",
        "sugerencias = spell.candidates(palabra)\n",
        "print(f\"Sugerencias para '{palabra}':\", sugerencias)\n",
        "print(f\"Sugerencias para 'corredoress':\", spell.candidates('corredoress'))\n",
        "print(f\"Sugerencias para 'corian':\", spell.candidates('corian'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7IiWkTko7B6",
        "outputId": "5cb844f0-54aa-4cf1-8ed3-c9187467613d"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sugerencias para 'corrienndo': {'corrigendo', 'correinado'}\n",
            "Sugerencias para 'corredoress': None\n",
            "Sugerencias para 'corian': {'coria', 'coriano', 'coriana', 'corion'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Agregar palabras personalizadas al diccionario\n",
        "spell.word_frequency.load_words(['corriendo', 'corrían', 'corredores'])\n",
        "texto = \"corrienddo rapidamente los corredoress corian\"\n",
        "correcciones = [spell.correction(token) for token in texto.split()]\n",
        "print(\"Texto corregido:\", \" \".join(correcciones))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3FfbtesRo8nC",
        "outputId": "1237c9a6-2e92-497e-9aae-03c81fb97fb0"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto corregido: corriendo rápidamente los corredores coria\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TextBlob"
      ],
      "metadata": {
        "id": "PhXu3h8InWtj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "texto = \"Corrienddo, corí rapdamente y los corredoress corian.\"\n",
        "blob = TextBlob(texto)\n",
        "\n",
        "# Corrección ortográfica\n",
        "texto_corregido = str(blob.correct())\n",
        "print(\"Texto corregido:\", texto_corregido)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxe2t2tknMSt",
        "outputId": "067cb27c-cdef-4968-d6fc-af0ac4c864bd"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto corregido: Corrienddo, cord rapdamente y los corredoress corn.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Uso de diccionarios (difflib)"
      ],
      "metadata": {
        "id": "ZTDF6ZvundOW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import difflib\n",
        "\n",
        "diccionario = [\"corriendo\", \"corrí\", \"rápidamente\", \"corredores\", \"corrían\"]\n",
        "texto = \"Corrienddo rapdamente los corredoress corian.\"\n",
        "tokens = texto.lower().split()\n",
        "\n",
        "# Buscar la palabra más cercana en el diccionario\n",
        "corregido = [difflib.get_close_matches(token, diccionario, n=1, cutoff=0.8)[0]\n",
        "             if difflib.get_close_matches(token, diccionario, n=1, cutoff=0.8) else token\n",
        "             for token in tokens]\n",
        "\n",
        "print(\"Texto corregido:\", \" \".join(corregido))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zqtpq3JUnho7",
        "outputId": "580437a5-d2a4-464d-dc67-c771e91e9099"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto corregido: corriendo rápidamente los corredores corian.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Emojis\n",
        "Convertir Emojis a Texto:\n",
        "En análisis de sentimientos o redes sociales, los emojis pueden expresar emociones.\n"
      ],
      "metadata": {
        "id": "i8DvrLcCWAzo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install emoji"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fgwoBciWJhL",
        "outputId": "f935bce2-0502-49fc-fb43-811513ccc1de"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
            "Downloading emoji-2.14.1-py3-none-any.whl (590 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/590.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/590.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.14.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import emoji\n",
        "\n",
        "texto = \"¡Este producto es increíble! 😍👍\"\n",
        "texto_convertido = emoji.demojize(texto)\n",
        "print(\"Texto con emojis convertidos:\", texto_convertido)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZLQ3lQGWGbG",
        "outputId": "e458560d-293e-4e9b-8fd0-921bad6e2b22"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto con emojis convertidos: ¡Este producto es increíble! :smiling_face_with_heart-eyes::thumbs_up:\n"
          ]
        }
      ]
    }
  ]
}